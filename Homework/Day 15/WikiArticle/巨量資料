巨量資料（英語：Big data[1][2][3]，中國大陸、香港稱作大數據），指的是在傳統數據處理應用軟體不足以處理的大或複雜的數據集的術語[4][5]。

巨量資料也可以定義為來自各種來源的大量非結構化或結構化數據。從學術角度而言，巨量資料的出現促成廣泛主題的新穎研究。這也導致各種巨量資料統計方法的發展。巨量資料並沒有統計學的抽樣方法；它只是觀察和追蹤發生的事情。因此，巨量資料通常包含的數據大小超出傳統軟體在可接受的時間內處理的能力。由於近期的技術進步，發布新數據的便捷性以及全球大多數政府對高透明度的要求，巨量資料分析在現代研究中越來越突出。[6] [7]

截至2012年 (2012-Missing required parameter 1=month!)[update]，技術上可在合理時間內分析處理的資料集大小單位為艾位元組（EB）[8]。在許多領域，由於資料集過度龐大，科學家經常在分析處理上遭遇限制和阻礙；這些領域包括氣象學、基因組學[9]、神經網路體學、複雜的物理類比[10]，以及生物和環境研究[11]。這樣的限制也對網路搜尋、金融與經濟資訊學造成影響。資料集大小增長的部分原因來自於資訊持續從各種來源被廣泛收集，這些來源包括搭載感測裝置的行動裝置、高空感測科技（遙感）、軟體記錄、相機、麥克風、無線射頻辨識（RFID）和無線感測網路。自1980年代起，現代科技可儲存資料的容量每40個月即增加一倍[12]；截至2012年 (2012-Missing required parameter 1=month!)[update]，全世界每天產生2.5艾位元組（2.5×1018位元組）的資料[13]。

巨量資料幾乎無法使用大多數的資料庫管理系統處理，而必須使用「在數十、數百甚至數千台伺服器上同時平行運行的軟體」（電腦叢集是其中一種常用方式）[14]。巨量資料的定義取決於持有資料組的機構之能力，以及其平常用來處理分析資料的軟體之能力。「對某些組織來說，第一次面對數百GB的資料集可能讓他們需要重新思考資料管理的選項。對於其他組織來說，資料集可能需要達到數十或數百TB才會對他們造成困擾。」[15]

隨著巨量資料被越來越多的提及，有些人驚呼巨量資料時代已經到來了，2012年《紐約時報》的一篇專欄中寫到，「巨量資料」時代已經降臨，在商業、經濟及其他領域中，決策將日益基於數據和分析而作出，而並非基於經驗和直覺。但是並不是所有人都對巨量資料感興趣，有些人甚至認為這是商學院或諮詢公司用來譁眾取寵的時髦術語(buzzword)，看起來很新穎，但只是把傳統重新包裝，之前在學術研究或者政策決策中也有巨量資料的支撐，巨量資料並不是一件新興事物。

巨量資料時代的來臨帶來無數的機遇，但是與此同時個人或機構的隱私權也極有可能受到衝擊，巨量資料包含各種個人資訊資料，現有的隱私保護法律或政策無力解決這些新出現的問題。有人提出，巨量資料時代，個人是否擁有「被遺忘權」，被遺忘權即是否有權利要求資料商不保留自己的某些資訊，巨量資料時代資訊為某些網際網路巨頭所控制，但是資料商收集任何資料未必都獲得使用者的許可，其對資料的控制權不具有合法性。2014年5月13日歐盟法院就「被遺忘權」（Case of Right to be Forgotten）一案作出裁定，判決Google應根據使用者請求刪除不完整的、無關緊要的、不相關的資料以保證資料不出現在搜尋結果中。這說明在巨量資料時代，加強對使用者個人權利的尊重才是時勢所趨的潮流。

巨量資料由巨型數據集（英語：Data set）組成，這些數據集大小常超出人類在可接受時間下的收集（英語：data acquisition）、庋用（英語：data curation）、管理和處理能力[16]。巨量資料的大小經常改變，截至2012年 (2012-Missing required parameter 1=month!)[update]，單一資料集的大小從數太位元組（TB）至數十兆億位元組（PB）不等。

在一份2001年的研究與相關的演講中[17]，麥塔集團（META Group，現為高德納）分析員道格·萊尼（Doug Laney）指出數據長的挑戰和機遇有三個方向：量（Volume，數據大小）、速（Velocity，資料輸入輸出的速度）與多變（Variety，多樣性），合稱「3V」或「3Vs」。高德納與現在大部份巨量資料產業中的公司，都繼續使用3V來描述巨量資料[18]。高德納於2012年修改對巨量資料的定義：「巨量資料是大量、高速、及/或多變的資訊資產，它需要新型的處理方式去促成更強的決策能力、洞察力與最佳化處理[原文 1][19]。」另外，有機構在3V之外定義第4個V：真實性（Veracity）為第四特點[20]。

巨量資料必須藉由計算機對資料進行統計、比對、解析方能得出客觀結果。美國在2012年就開始著手巨量資料，歐巴馬更在同年投入2億美金在巨量資料的開發中，更強調巨量資料會是之後的未來石油。

資料探勘（data mining）則是在探討用以解析巨量資料的方法。

巨量資料需要特殊的技術，以有效地處理大量的容忍經過時間內的資料。適用於特殊巨量資料的技術，包括大規模並列處理（MPP）資料庫、資料探勘、分散式檔案系統、分散式資料庫、雲端運算平台、網際網路和可延伸的儲存系統。

巨量資料取得之來源影響其應用之效益與品質，依照取得的直接程度一般可分為三種[21][22]：

巨量資料的應用範例包括大科學、RFID、感測裝置網路、天文學、大氣學、交通運輸、基因組學、生物學、大社會資料分析[26]、網際網路檔案處理、製作網際網路搜尋引擎索引、通信記錄明細、軍事偵查、金融巨量資料，醫療巨量資料，社群網路、通勤時間預測、醫療記錄、相片圖像和影像封存、大規模的電子商務等[27]。

大型強子對撞機中有1億5000萬個感測器，每秒傳送4000萬次的資料。實驗中每秒產生將近6億次的對撞，在過濾去除99.999%的撞擊資料後，得到約100次的有用撞擊資料[28][29][30]。

將撞擊結果資料過濾處理後僅記錄0.001%的有用資料，全部四個對撞機的資料量複製前每年產生25拍位元組（PB），複製後為200拍位元組。

如果將所有實驗中的資料在不過濾的情況下全部記錄，資料量將會變得過度龐大且極難處理。每年資料量在複製前將會達到1.5億拍位元組，等於每天有近500艾位元組（EB）的資料量。這個數字代表每天實驗將產生相當於500垓（5×1020）位元組的資料，是全世界所有資料來源總和的200倍。

國際衛生學教授漢斯·羅斯林使用「Trendalyzer」工具軟體呈現兩百多年以來全球人類的人口統計資料，跟其他數據交叉比對，例如收入、宗教、能源使用量等。

目前，已開發國家的政府部門開始推廣巨量資料的應用。2012年歐巴馬政府投資近兩億美元開始推行《巨量資料的研究與發展計劃》，本計劃涉及美國國防部、美國衛生與公共服務部門等多個聯邦部門和機構，意在通過提高從大型複雜的的資料中提取知識的能力，進而加快科學和工程的開發，保障國家安全。

中國政府計劃建立全面的個人信用評分體系，其包含不少對個人行為的評定，有關指標會影響到個人貸款、工作、簽證等生活活動。高科技公司在被政治介入為其目的服務，個人的大部分行為和社交關係受掌控，幾乎無人可免於被納入個人信用評價體系的監視中[31]。除獲取網路數據外，中國政府還希望從科技公司獲得分類和分析資訊的雲端計算能力，透過城市監視攝錄影機、智慧型手機、政府資料庫等蒐集數據，以建造智慧型城市和安全城市。人權觀察駐香港研究員王松蓮指出，整個安全城市構想無非是一個龐大的監視項目[32]。

大資料產生的背景離不開Facebook等社群網路的興起，人們每天通過這種自媒體傳播資訊或者溝通交流，由此產生的資訊被網路記錄下來，社會學家可以在這些資料的基礎上分析人類的行為模式、交往方式等。美國的涂爾幹計劃就是依據個人在社群網路上的資料分析其自殺傾向，該計劃從美軍退役士兵中揀選受試者，透過Facebook的行動app收集資料，並將使用者的活動資料傳送到一個醫療資料庫。收集完成的資料會接受人工智慧系統分析，接著利用預測程式來即時監視受測者是否出現一般認為具傷害性的行為。

運用資料探勘技術，分析網路聲量，以了解客戶行為、市場需求，做行銷策略參考與商業決策支援，或是應用於品牌管理，經營網路口碑、掌握負面事件等。如電信業者透過品牌的網路討論資料，即時找出負面事件進行處理，減低負面討論在網路擴散後所可能引發的形象危害。[39]

巨量資料的出現提升了對資訊管理專家的需求，Software AG、Oracle、IBM、微軟、SAP、易安信、惠普和戴爾已在多間資料管理分析專門公司上花費超過150億美元。在2010年，資料管理分析產業市值超過1,000億美元，並以每年將近10%的速度成長，是整個軟體產業成長速度的兩倍[34]。

經濟的開發成長促進了密集資料科技的使用。全世界共有約46億的行動電話使用者，並有10至20億人連結網際網路[34]。自1990年起至2005年間，全世界有超過10億人進入中產階級，收入的增加造成了識字率的提升，更進而帶動資訊量的成長。全世界透過電信網路交換資訊的容量在1986年為281兆億位元組（PB），1993年為471兆億位元組，2000年時增長為2.2艾位元組（EB），在2007年則為65艾位元組[12]。根據預測，在2013年網際網路每年的資訊流量將會達到667艾位元組[34]。

